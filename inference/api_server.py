#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„æ¨ç†APIæœåŠ¡å™¨
ç«¯å£: 5006
åŠŸèƒ½: æä¾›æµå¼æ¨ç†æœåŠ¡ï¼Œè°ƒç”¨vLLMæœåŠ¡å™¨è¿›è¡Œæ·±åº¦ç ”ç©¶
"""

import os
import json
import asyncio
from typing import Dict, Generator, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

from streaming_agent import StreamingReactAgent

# é…ç½®
API_PORT = int(os.getenv('API_PORT', 5006))
VLLM_PORT = int(os.getenv('PLANNING_PORTS', 6001))
MODEL_PATH = os.getenv('MODEL_PATH', '/path/to/your/model')

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Tongyi DeepResearch API",
    description="æ·±åº¦ç ”ç©¶æ¨ç†APIæœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    question: str
    temperature: Optional[float] = 0.85
    top_p: Optional[float] = 0.95
    presence_penalty: Optional[float] = 1.1
    max_tokens: Optional[int] = 10000

class ChatResponse(BaseModel):
    type: str
    content: str
    timestamp: str
    round: Optional[int] = None
    tool_name: Optional[str] = None
    tool_args: Optional[Dict] = None
    result: Optional[str] = None
    code: Optional[str] = None

# å…¨å±€å˜é‡
agent = None

def initialize_agent():
    """åˆå§‹åŒ–æ¨ç†ä»£ç†"""
    global agent
    
    # é…ç½®LLMå‚æ•°
    llm_config = {
        "model": MODEL_PATH,
        "generate_cfg": {
            "temperature": 0.85,
            "top_p": 0.95,
            "presence_penalty": 1.1,
            "max_tokens": 10000
        }
    }
    
    # åˆå§‹åŒ–ä»£ç†
    agent = StreamingReactAgent(llm=llm_config)
    print(f"âœ… æ¨ç†ä»£ç†åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ“¡ vLLMæœåŠ¡å™¨åœ°å€: http://localhost:{VLLM_PORT}/v1")
    print(f"ğŸ¤– æ¨¡å‹è·¯å¾„: {MODEL_PATH}")

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    print("ğŸš€ å¯åŠ¨Tongyi DeepResearch APIæœåŠ¡å™¨...")
    initialize_agent()
    print(f"ğŸŒ APIæœåŠ¡å™¨å°†åœ¨ç«¯å£ {API_PORT} ä¸Šè¿è¡Œ")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Tongyi DeepResearch API Server",
        "version": "1.0.0",
        "status": "running",
        "vllm_port": VLLM_PORT,
        "model_path": MODEL_PATH
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "agent_initialized": agent is not None
    }

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼èŠå¤©æ¥å£"""
    if agent is None:
        raise HTTPException(status_code=500, detail="æ¨ç†ä»£ç†æœªåˆå§‹åŒ–")
    
    try:
        # æ›´æ–°ä»£ç†é…ç½®
        agent.llm_generate_cfg.update({
            "temperature": request.temperature,
            "top_p": request.top_p,
            "presence_penalty": request.presence_penalty,
            "max_tokens": request.max_tokens
        })
        
        def generate_stream():
            """ç”Ÿæˆæµå¼å“åº”"""
            try:
                for event in agent.stream_run(request.question, VLLM_PORT):
                    # è½¬æ¢ä¸ºJSONæ ¼å¼
                    response_data = {
                        "type": event.get("type", "unknown"),
                        "content": event.get("content", ""),
                        "timestamp": event.get("timestamp", datetime.now().isoformat())
                    }
                    
                    # æ·»åŠ å¯é€‰å­—æ®µ
                    if "round" in event:
                        response_data["round"] = event["round"]
                    if "tool_name" in event:
                        response_data["tool_name"] = event["tool_name"]
                    if "tool_args" in event:
                        response_data["tool_args"] = event["tool_args"]
                    if "result" in event:
                        response_data["result"] = event["result"]
                    if "code" in event:
                        response_data["code"] = event["code"]
                    
                    # å‘é€æ•°æ®
                    yield f"data: {json.dumps(response_data, ensure_ascii=False)}\n\n"
                    
            except Exception as e:
                error_data = {
                    "type": "error",
                    "content": f"æµå¼å¤„ç†å‡ºé”™: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")

@app.post("/chat")
async def chat(request: ChatRequest):
    """éæµå¼èŠå¤©æ¥å£ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    if agent is None:
        raise HTTPException(status_code=500, detail="æ¨ç†ä»£ç†æœªåˆå§‹åŒ–")
    
    try:
        # æ›´æ–°ä»£ç†é…ç½®
        agent.llm_generate_cfg.update({
            "temperature": request.temperature,
            "top_p": request.top_p,
            "presence_penalty": request.presence_penalty,
            "max_tokens": request.max_tokens
        })
        
        # æ”¶é›†æ‰€æœ‰äº‹ä»¶
        events = []
        for event in agent.stream_run(request.question, VLLM_PORT):
            events.append(event)
        
        return {
            "question": request.question,
            "events": events,
            "total_events": len(events),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    print("ğŸ”§ å¯åŠ¨é…ç½®:")
    print(f"   APIç«¯å£: {API_PORT}")
    print(f"   vLLMç«¯å£: {VLLM_PORT}")
    print(f"   æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print()
    
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=API_PORT,
        reload=False,
        log_level="info"
    )





