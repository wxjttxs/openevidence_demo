import json
import json5
import os
import time
import asyncio
import logging
from typing import Dict, Iterator, List, Optional, Generator
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from react_agent import MultiTurnReactAgent, TOOL_MAP, MAX_LLM_CALL_PER_RUN, today_date
from prompt import SYSTEM_PROMPT
from answer_system import AnswerJudgmentSystem
from department_classifier import classify_question_and_get_dataset_ids

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class StreamingReactAgent(MultiTurnReactAgent):
    """æµå¼æ¨ç†ä»£ç†ï¼Œæ”¯æŒå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹å’Œå·¥å…·è°ƒç”¨"""
    
    def __init__(self, llm=None, function_list=None, **kwargs):
        logger.debug(f"Initializing StreamingReactAgent with llm: {llm}, function_list: {function_list}")
        
        if llm is None:
            raise ValueError("llm parameter is required")
        
        self.llm_generate_cfg = llm["generate_cfg"]
        self.llm_model = llm["model"]
        self.llm_base_url = llm.get("base_url", "http://127.0.0.1:6001/v1")
        self.llm_api_key = llm.get("api_key", "EMPTY")
        
        # ä¸è°ƒç”¨çˆ¶ç±»çš„__init__ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å…¶ä¸­çš„éƒ¨åˆ†åŠŸèƒ½
        self.function_list = function_list or []
        
        # åˆå§‹åŒ–ç­”æ¡ˆåˆ¤æ–­ç³»ç»Ÿ
        self.answer_system = AnswerJudgmentSystem()
        
    def call_server(self, msgs, max_tries=3):
        """è°ƒç”¨LLMæœåŠ¡å™¨"""
        from openai import OpenAI, APIError, APIConnectionError, APITimeoutError
        import random

        client = OpenAI(
            api_key=self.llm_api_key,
            base_url=self.llm_base_url,
            timeout=600.0,
        )

        base_sleep_time = 1 
        
        for attempt in range(max_tries):
            try:
                logger.debug(f"--- Attempting to call the service, try {attempt + 1}/{max_tries} ---")
                chat_response = client.chat.completions.create(
                    model=self.llm_model,
                    messages=msgs,
                    stop=["\n<tool_response>", "<tool_response>"],
                    temperature=self.llm_generate_cfg.get('temperature', 0.6),
                    top_p=self.llm_generate_cfg.get('top_p', 0.95),
                    logprobs=True,
                    max_tokens=8000,
                    presence_penalty=self.llm_generate_cfg.get('presence_penalty', 1.1)
                )
                content = chat_response.choices[0].message.content
                if content and content.strip():
                    logger.debug("--- Service call successful, received a valid response ---")
                    return content.strip()
                else:
                    logger.debug(f"Warning: Attempt {attempt + 1} received an empty response.")

            except (APIError, APIConnectionError, APITimeoutError) as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an API or network error: {e}")
            except Exception as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an unexpected error: {e}")

            if attempt < max_tries - 1:
                sleep_time = base_sleep_time * (2 ** attempt) + random.uniform(0, 1)
                sleep_time = min(sleep_time, 30) 
                
                logger.debug(f"Retrying in {sleep_time:.2f} seconds...")
                time.sleep(sleep_time)
            else:
                logger.debug("Error: All retry attempts have been exhausted. The call has failed.")
        
        return f"vllm server error!!!"
    
    def call_server_stream(self, msgs, max_tries=3, enable_thinking=True):
        """
        æµå¼è°ƒç”¨LLMæœåŠ¡å™¨ï¼Œæ”¯æŒé˜¿é‡Œäº‘æ¨¡å‹çš„thinkingæ¨¡å¼
        
        Args:
            msgs: æ¶ˆæ¯åˆ—è¡¨
            max_tries: æœ€å¤§é‡è¯•æ¬¡æ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆé˜¿é‡Œäº‘æ¨¡å‹ä¸“ç”¨ï¼‰
            
        Yields:
            dict: åŒ…å« type å’Œ content çš„å­—å…¸
                - type: 'reasoning' (æ€è€ƒè¿‡ç¨‹) æˆ– 'content' (å›ç­”å†…å®¹)
                - content: æ–‡æœ¬å†…å®¹
        """
        from openai import OpenAI, APIError, APIConnectionError, APITimeoutError
        import random

        client = OpenAI(
            api_key=self.llm_api_key,
            base_url=self.llm_base_url,
            timeout=600.0,
        )

        base_sleep_time = 1 
        
        for attempt in range(max_tries):
            try:
                logger.debug(f"--- Attempting to call the service (stream), try {attempt + 1}/{max_tries} ---")
                
                # æ„å»ºAPIè°ƒç”¨å‚æ•°
                api_params = {
                    "model": self.llm_model,
                    "messages": msgs,
                    "stop": ["\n<tool_response>", "<tool_response>"],
                    "temperature": self.llm_generate_cfg.get('temperature', 0.6),
                    "top_p": self.llm_generate_cfg.get('top_p', 0.95),
                    "max_tokens": 1000,
                    "presence_penalty": self.llm_generate_cfg.get('presence_penalty', 1.1),
                    "stream": True
                }
                
                # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œæ·»åŠ extra_bodyå‚æ•°ï¼ˆé˜¿é‡Œäº‘æ¨¡å‹ä¸“ç”¨ï¼‰
                if enable_thinking and 'qwen' in self.llm_model.lower():
                    api_params["extra_body"] = {
                        "enable_thinking": True,
                        "thinking_budget": 1000  # æœ€å¤§æ€è€ƒtokenæ•°
                    }
                    logger.info(f"ğŸ§  å·²å¯ç”¨æ€è€ƒæ¨¡å¼ (thinking_budget=1000)")
                
                stream = client.chat.completions.create(**api_params)
                
                accumulated_reasoning = ""
                accumulated_content = ""
                has_reasoning = False
                has_content = False
                
                for chunk in stream:
                    if not chunk.choices:
                        continue
                    
                    delta = chunk.choices[0].delta
                    
                    # å¤„ç†æ€è€ƒå†…å®¹ (reasoning_content)
                    if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                        has_reasoning = True
                        accumulated_reasoning += delta.reasoning_content
                        yield {
                            "type": "reasoning",
                            "content": delta.reasoning_content,
                            "accumulated": accumulated_reasoning
                        }
                    
                    # å¤„ç†å›ç­”å†…å®¹ (content)
                    if hasattr(delta, "content") and delta.content:
                        has_content = True
                        accumulated_content += delta.content
                        yield {
                            "type": "content",
                            "content": delta.content,
                            "accumulated": accumulated_content
                        }
                
                if has_reasoning or has_content:
                    logger.info(f"âœ… æµå¼è°ƒç”¨æˆåŠŸ - æ€è€ƒ: {len(accumulated_reasoning)}å­—, å›ç­”: {len(accumulated_content)}å­—")
                    return  # æˆåŠŸå®Œæˆ
                else:
                    logger.debug(f"Warning: Attempt {attempt + 1} received an empty response.")

            except (APIError, APIConnectionError, APITimeoutError) as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an API or network error: {e}")
            except Exception as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an unexpected error: {e}")

            if attempt < max_tries - 1:
                sleep_time = base_sleep_time * (2 ** attempt) + random.uniform(0, 1)
                sleep_time = min(sleep_time, 30) 
                
                logger.debug(f"Retrying in {sleep_time:.2f} seconds...")
                time.sleep(sleep_time)
            else:
                logger.debug("Error: All retry attempts have been exhausted. The call has failed.")
        
        yield {"type": "error", "content": "vllm server error!!!"}
        
    def count_tokens(self, messages, model="gpt-4o"):
        """è®¡ç®—tokenæ•°é‡"""
        try: 
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.llm_model) 
        except Exception as e: 
            import tiktoken
            tokenizer = tiktoken.encoding_for_model(model)
        
        from qwen_agent.llm.schema import Message
        from qwen_agent.utils.utils import build_text_completion_prompt
        
        full_message = [Message(**x) for x in messages]
        full_prompt = build_text_completion_prompt(full_message, allow_special=True)
        
        return len(tokenizer.encode(full_prompt))
        
    def custom_call_tool(self, tool_name: str, tool_args: dict, **kwargs):
        """è°ƒç”¨å·¥å…·"""
        logger.debug(f"[DEBUG] custom_call_tool called with: tool_name={tool_name}, tool_args={tool_args}")
        
        if tool_name in TOOL_MAP:
            if "python" in tool_name.lower():
                result = TOOL_MAP['PythonInterpreter'].call(tool_args)
            elif tool_name == "parse_file":
                params = {"files": tool_args["files"]}
                
                raw_result = asyncio.run(TOOL_MAP[tool_name].call(params, file_root_path="./eval_data/file_corpus"))
                result = raw_result

                if not isinstance(raw_result, str):
                    result = str(raw_result)
            else:
                # ç›´æ¥ä¼ é€’tool_argsï¼Œä¸è¦æ·»åŠ é¢å¤–çš„paramsåŒ…è£…
                raw_result = TOOL_MAP[tool_name].call(tool_args, **kwargs)
                result = raw_result
            return result

        else:
            return f"Error: Tool {tool_name} not found"
        
    def stream_run(self, question: str, cancelled: dict = None, history_messages: list = None) -> Generator[Dict, None, None]:
        """
        æµå¼è¿è¡Œæ¨ç†è¿‡ç¨‹ï¼Œå®æ—¶è¾“å‡ºå„ä¸ªé˜¶æ®µçš„ä¿¡æ¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            cancelled: å–æ¶ˆæ ‡è®°å­—å…¸ {"value": False}ï¼Œå½“è®¾ç½®ä¸º True æ—¶ä¸­æ–­å¤„ç†
            history_messages: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
            
        Yields:
            Dict: åŒ…å«å½“å‰é˜¶æ®µä¿¡æ¯çš„å­—å…¸
        """
        # åˆå§‹åŒ– cancelled æ ‡è®°
        if cancelled is None:
            cancelled = {"value": False}
        if history_messages is None:
            history_messages = []
        
        logger.info(f"=== StreamingReactAgent.stream_run START ===")
        logger.debug(f"Question: {question}")
        logger.debug(f"LLM Model: {self.llm_model}")
        logger.debug(f"History messages: {len(history_messages)} messages")
        
        start_time = time.time()
        self.user_prompt = question
        
        # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
        system_prompt = SYSTEM_PROMPT + str(today_date())
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆæ’é™¤systemæ¶ˆæ¯ï¼‰
        for msg in history_messages:
            if msg.get("role") != "system":
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
        
        # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
        messages.append({"role": "user", "content": question})
        
        init_event = {
            "type": "init",
            "content": f"å¼€å§‹å¤„ç†é—®é¢˜...",
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Yielding init event: {init_event}")
        yield init_event
        
        # é™åˆ¶æ£€ç´¢å¾ªç¯æœ€å¤š3æ¬¡
        MAX_RETRIEVAL_ROUNDS = 3
        num_llm_calls_available = MAX_LLM_CALL_PER_RUN
        round_num = 0
        retrieval_round_num = 0  # æ£€ç´¢è½®æ¬¡è®¡æ•°
        previous_search_keywords = []  # è®°å½•ä¹‹å‰ä½¿ç”¨çš„æ£€ç´¢å…³é”®è¯
        
        logger.debug(f"Starting main loop, max retrieval rounds: {MAX_RETRIEVAL_ROUNDS}")
        
        while num_llm_calls_available > 0:
            # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
            if cancelled["value"]:
                cancelled_event = {
                    "type": "cancelled",
                    "content": "æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å¤„ç†",
                    "timestamp": datetime.now().isoformat()
                }
                logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢æ¨ç†å¾ªç¯")
                yield cancelled_event
                
                completed_event = {
                    "type": "completed",
                    "content": "å®¢æˆ·ç«¯æ–­å¼€ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (cancelled): {completed_event}")
                yield completed_event
                return
            
            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > 150 * 60:  # 150åˆ†é’Ÿ
                timeout_event = {
                    "type": "timeout",
                    "content": "æ¨ç†è¶…æ—¶ï¼ˆ150åˆ†é’Ÿï¼‰",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding timeout event: {timeout_event}")
                yield timeout_event
                
                completed_event = {
                    "type": "completed",
                    "content": "æ¨ç†è¶…æ—¶ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (timeout): {completed_event}")
                yield completed_event
                return
                
            round_num += 1
            num_llm_calls_available -= 1
            
            round_start_event = {
                "type": "round_start",
                "content": f"ç¬¬ {round_num} è½®æ¨ç†å¼€å§‹",
                "round": round_num,
                "timestamp": datetime.now().isoformat()
            }
            logger.debug(f"Yielding round_start event: {round_start_event}")
            yield round_start_event
            
            # æ£€æŸ¥æ£€ç´¢è½®æ¬¡é™åˆ¶ï¼ˆåœ¨è°ƒç”¨æ£€ç´¢å·¥å…·å‰æ£€æŸ¥ï¼‰
            if retrieval_round_num >= MAX_RETRIEVAL_ROUNDS:
                # å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡ï¼Œè¿”å›æŠ±æ­‰æ¶ˆæ¯
                logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡ {MAX_RETRIEVAL_ROUNDS}ï¼Œæ— æ³•æ‰¾åˆ°ç­”æ¡ˆ")
                
                no_answer_event = {
                    "type": "final_answer",
                    "content": "æŠ±æ­‰ï¼Œç»è¿‡å¤šè½®æ£€ç´¢åï¼Œæˆ‘æ— æ³•åŸºäºç°æœ‰çŸ¥è¯†åº“æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨ï¼š\n\n1. å°è¯•ä½¿ç”¨æ›´å…·ä½“æˆ–ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n2. å°†é—®é¢˜æ‹†åˆ†ä¸ºæ›´å°çš„å­é—®é¢˜\n3. å¦‚æœå¯èƒ½ï¼Œæä¾›æ›´å¤šç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯",
                    "timestamp": datetime.now().isoformat(),
                    "is_streaming": False
                }
                logger.debug(f"Yielding no_answer event: {no_answer_event}")
                yield no_answer_event
                
                completed_event = {
                    "type": "completed",
                    "content": f"å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡ï¼ˆ{MAX_RETRIEVAL_ROUNDS}æ¬¡ï¼‰ï¼Œæœªèƒ½æ‰¾åˆ°ç­”æ¡ˆ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (max rounds): {completed_event}")
                yield completed_event
                logger.info(f"=== StreamingReactAgent.stream_run MAX_ROUNDS_REACHED ===")
                return
            
            # è°ƒç”¨LLM
            thinking_start_time = time.time()
            
            # æ„å»ºæ€è€ƒæç¤ºï¼Œå¼ºè°ƒä½¿ç”¨ä¸åŒçš„æ£€ç´¢å…³é”®è¯
            thinking_hint = ""
            if previous_search_keywords:
                keywords_list = "ã€".join([f'"{kw}"' for kw in previous_search_keywords])
                thinking_hint = f"\n\nã€é‡è¦æç¤ºã€‘è¿™æ˜¯ç¬¬ {retrieval_round_num + 1} è½®æ£€ç´¢ã€‚ä¹‹å‰å·²ä½¿ç”¨è¿‡ä»¥ä¸‹æ£€ç´¢å…³é”®è¯ï¼š{keywords_list}ã€‚\nè¯·åŠ¡å¿…ä½¿ç”¨**ä¸åŒçš„æ£€ç´¢å…³é”®è¯**æˆ–**ä¸åŒçš„æ£€ç´¢è§’åº¦**ï¼Œç¡®ä¿æœ¬æ¬¡æ£€ç´¢èƒ½è·å¾—ä¸åŒçš„ç»“æœã€‚é¿å…é‡å¤ä½¿ç”¨ç›¸åŒçš„å…³é”®è¯ã€‚"
            
            thinking_content = "æ­£åœ¨æ€è€ƒ..." + thinking_hint
            thinking_start_event = {
                "type": "thinking_start",
                "content": thinking_content,
                "timestamp": datetime.now().isoformat()
            }
            logger.debug(f"Yielding thinking_start event (round {retrieval_round_num + 1}/{MAX_RETRIEVAL_ROUNDS}): {thinking_start_event}")
            yield thinking_start_event
            
            try:
                logger.debug(f"Calling LLM server (stream) - Model: {self.llm_model}, Retrieval Round: {retrieval_round_num + 1}/{MAX_RETRIEVAL_ROUNDS}")
                
                # å¦‚æœä¹‹å‰æœ‰æ£€ç´¢å¤±è´¥ï¼Œåœ¨system messageä¸­æ·»åŠ æç¤º
                current_messages = messages.copy()
                if previous_search_keywords and retrieval_round_num > 0:
                    # æ·»åŠ æç¤ºæ¶ˆæ¯ï¼Œå¼ºè°ƒä½¿ç”¨ä¸åŒçš„å…³é”®è¯
                    keyword_hint = f"æ³¨æ„ï¼šè¿™æ˜¯ç¬¬ {retrieval_round_num + 1} è½®æ£€ç´¢ã€‚ä¹‹å‰çš„æ£€ç´¢æœªèƒ½æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ã€‚å·²ä½¿ç”¨è¿‡çš„æ£€ç´¢å…³é”®è¯åŒ…æ‹¬ï¼š{', '.join(previous_search_keywords)}ã€‚è¯·åŠ¡å¿…ä½¿ç”¨**å®Œå…¨ä¸åŒçš„æ£€ç´¢å…³é”®è¯**æˆ–**ä¸åŒçš„æ£€ç´¢è§’åº¦**ï¼Œç¡®ä¿æœ¬æ¬¡æ£€ç´¢èƒ½è·å¾—ä¸åŒçš„ç»“æœã€‚"
                    # å°†æç¤ºæ’å…¥åˆ°useræ¶ˆæ¯ä¸­
                    if len(current_messages) > 0:
                        # åœ¨æœ€åä¸€æ¡useræ¶ˆæ¯åæ·»åŠ æç¤º
                        last_user_idx = -1
                        for i in range(len(current_messages) - 1, -1, -1):
                            if current_messages[i]["role"] == "user":
                                last_user_idx = i
                                break
                        if last_user_idx >= 0:
                            current_messages[last_user_idx]["content"] += "\n\n" + keyword_hint
                        else:
                            # å¦‚æœæ²¡æœ‰useræ¶ˆæ¯ï¼Œæ·»åŠ ä¸€æ¡
                            current_messages.append({"role": "user", "content": keyword_hint})
                
                # ä½¿ç”¨æ–°çš„æµå¼APIï¼Œæ”¯æŒé˜¿é‡Œäº‘thinkingæ¨¡å¼
                reasoning_content = ""  # å®Œæ•´æ€è€ƒè¿‡ç¨‹
                answer_content = ""  # å®Œæ•´å›ç­”å†…å®¹
                is_answering = False  # æ˜¯å¦è¿›å…¥å›ç­”é˜¶æ®µ
                
                for chunk_data in self.call_server_stream(current_messages, enable_thinking=True):
                    # åœ¨æµå¼æ¥æ”¶è¿‡ç¨‹ä¸­æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                    if cancelled["value"]:
                        logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢LLMæµå¼æ¥æ”¶")
                        return
                    
                    chunk_type = chunk_data.get("type")
                    chunk_content = chunk_data.get("content", "")
                    
                    # å¤„ç†æ€è€ƒå†…å®¹ï¼ˆreasoningï¼‰
                    if chunk_type == "reasoning":
                        reasoning_content = chunk_data.get("accumulated", reasoning_content)
                        # å®æ—¶å‘é€æ€è€ƒç‰‡æ®µ
                        if chunk_content.strip():
                            thinking_chunk_event = {
                                "type": "thinking_chunk",
                                "content": chunk_content,
                                "accumulated": reasoning_content,
                                "is_streaming": True,
                                "is_complete": False,
                                "timestamp": datetime.now().isoformat()
                            }
                            yield thinking_chunk_event
                    
                    # å¤„ç†å›ç­”å†…å®¹ï¼ˆcontentï¼‰
                    elif chunk_type == "content":
                        if not is_answering:
                            # ç¬¬ä¸€æ¬¡æ”¶åˆ°contentï¼Œè¯´æ˜æ€è€ƒé˜¶æ®µç»“æŸ
                            is_answering = True
                            
                            # å¦‚æœæœ‰æ€è€ƒå†…å®¹ï¼Œå‘é€æ€è€ƒå®Œæˆäº‹ä»¶
                            if reasoning_content.strip():
                                thinking_elapsed = time.time() - thinking_start_time
                                logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ€è€ƒè¿‡ç¨‹å®Œæˆï¼Œè€—æ—¶: {thinking_elapsed:.2f} ç§’")
                                
                                thinking_complete_event = {
                                    "type": "thinking",
                                    "content": reasoning_content.strip(),
                                    "is_streaming": False,
                                    "timestamp": datetime.now().isoformat(),
                                    "elapsed_time": f"{thinking_elapsed:.2f}ç§’"
                                }
                                # logger.info(f"ğŸ’­ æ€è€ƒå†…å®¹é•¿åº¦: {len(reasoning_content)} å­—")
                                yield thinking_complete_event
                            else:
                                logger.info(f"âš ï¸ æ¨¡å‹æœªè¾“å‡ºæ€è€ƒå†…å®¹ï¼ˆå¯èƒ½æ˜¯éthinkingæ¨¡å‹ï¼‰")
                        
                        # ç´¯ç§¯å›ç­”å†…å®¹
                        answer_content += chunk_content
                    
                    # å¤„ç†é”™è¯¯
                    elif chunk_type == "error":
                        raise Exception(chunk_content)
                
                # ä½¿ç”¨answer_contentä½œä¸ºæœ€ç»ˆcontent
                content = answer_content if answer_content else reasoning_content
                # logger.info(f"âœ… LLMå“åº”å®Œæˆ - æ€è€ƒ: {len(reasoning_content)}å­—, å›ç­”: {len(answer_content)}å­—")
                
                # æ¸…ç†tool_responseæ ‡è®°
                if '<tool_response>' in content:
                    pos = content.find('<tool_response>')
                    content = content[:pos]
                    
                messages.append({"role": "assistant", "content": content.strip()})
                logger.debug(f"Added assistant message to conversation")
                
                # æ£€æŸ¥å·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒå¤šä¸ªè¿ç»­çš„tool_callï¼‰
                if '<tool_call>' in content and '</tool_call>' in content:
                    # åœ¨æ‰§è¡Œå·¥å…·è°ƒç”¨å‰æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                    if cancelled["value"]:
                        cancelled_event = {
                            "type": "cancelled",
                            "content": "æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å¤„ç†",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œè·³è¿‡å·¥å…·è°ƒç”¨")
                        yield cancelled_event
                        
                        completed_event = {
                            "type": "completed",
                            "content": "å®¢æˆ·ç«¯æ–­å¼€ï¼Œæµç¨‹ç»“æŸ",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding completed event (cancelled before tool): {completed_event}")
                        yield completed_event
                        return
                    
                    # æå–æ‰€æœ‰tool_callï¼ˆæ”¯æŒå¤šä¸ªè¿ç»­çš„tool_callï¼‰
                    import re
                    tool_call_pattern = r'<tool_call>(.*?)</tool_call>'
                    tool_calls_raw = re.findall(tool_call_pattern, content, re.DOTALL)
                    
                    logger.debug(f"Found {len(tool_calls_raw)} tool call(s) in response")
                    
                    # å¦‚æœåªæœ‰ä¸€ä¸ªtool_callï¼Œä¸²è¡Œå¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    if len(tool_calls_raw) == 1:
                        tool_call_raw = tool_calls_raw[0].strip()
                        
                        tool_call_start_event = {
                            "type": "tool_call_start",
                            "content": f"å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call_raw[:100]}...",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding tool_call_start event: {tool_call_start_event}")
                        yield tool_call_start_event
                        
                        try:
                            if "python" in tool_call_raw.lower():
                                # Pythonä»£ç æ‰§è¡Œ
                                try:
                                    code_raw = content.split('<tool_call>')[1].split('</tool_call>')[0].split('<code>')[1].split('</code>')[0].strip()
                                    python_exec_event = {
                                        "type": "python_execution",
                                        "content": f"æ‰§è¡ŒPythonä»£ç :\n```python\n{code_raw}\n```",
                                        "code": code_raw,
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    logger.debug(f"Yielding python_execution event: {python_exec_event}")
                                    yield python_exec_event
                                    
                                    result = TOOL_MAP['PythonInterpreter'].call(code_raw)
                                    logger.debug(f"Python execution result: {result[:200]}...")
                                except Exception as e:
                                    result = f"[Python Interpreter Error]: {str(e)}"
                                    logger.debug(f"Python execution error: {result}")
                            else:
                                # å…¶ä»–å·¥å…·è°ƒç”¨
                                tool_call = json5.loads(tool_call_raw)
                                tool_name = tool_call.get('name', '')
                                tool_args = tool_call.get('arguments', {})
                                
                                tool_exec_event = {
                                    "type": "tool_execution",
                                    "content": f"è°ƒç”¨å·¥å…· {tool_name}ï¼Œå‚æ•°: {json.dumps(tool_args, indent=2, ensure_ascii=False)}",
                                    "tool_name": tool_name,
                                    "tool_args": tool_args,
                                    "timestamp": datetime.now().isoformat()
                                }
                                logger.debug(f"Yielding tool_execution event: {tool_exec_event}")
                                yield tool_exec_event
                                
                                retrieval_start_time = time.time()
                                logger.debug(f"Calling tool {tool_name} with args {tool_args}")
                                
                                # å¦‚æœæ˜¯æ£€ç´¢å·¥å…·ï¼Œè®°å½•æ£€ç´¢å…³é”®è¯å¹¶è‡ªåŠ¨è¡¥å……dataset_ids
                                if tool_name == "retrieval":
                                    retrieval_round_num += 1
                                    search_keyword = tool_args.get("question", "")
                                    if search_keyword:
                                        previous_search_keywords.append(search_keyword)
                                        logger.info(f"ğŸ” ç¬¬ {retrieval_round_num} è½®æ£€ç´¢ï¼Œå…³é”®è¯: {search_keyword}")
                                    
                                    # è‡ªåŠ¨æ ¹æ®é—®é¢˜åˆ¤æ–­ç§‘å®¤å¹¶è¡¥å……dataset_ids
                                    provided_dataset_ids = tool_args.get("dataset_ids", [])
                                    if not provided_dataset_ids or len(provided_dataset_ids) == 0:
                                        classification_result = classify_question_and_get_dataset_ids(self.user_prompt)
                                        auto_dataset_ids = classification_result["dataset_ids"]
                                        tool_args["dataset_ids"] = auto_dataset_ids
                                        logger.info(f"ğŸ“‹ LLMæœªæä¾›dataset_idsï¼Œè‡ªåŠ¨æ ¹æ®é—®é¢˜åˆ¤æ–­ç§‘å®¤: {classification_result['departments']}, ä½¿ç”¨dataset_ids: {auto_dataset_ids}")
                                    else:
                                        logger.info(f"ğŸ“‹ LLMå·²æä¾›dataset_ids: {provided_dataset_ids}")
                                
                                result = self.custom_call_tool(tool_name, tool_args)
                                retrieval_elapsed = time.time() - retrieval_start_time
                                logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {retrieval_elapsed:.2f} ç§’")
                                logger.debug(f"Tool result: {result[:200]}...")
                                
                                # å·¥å…·è°ƒç”¨å®Œæˆåæ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                                if cancelled["value"]:
                                    logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å·¥å…·ç»“æœå¤„ç†")
                                    return
                                
                        except Exception as e:
                            result = f'å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}'
                            tool_error_event = {
                                "type": "tool_error",
                                "content": result,
                                "timestamp": datetime.now().isoformat()
                            }
                            logger.debug(f"Yielding tool_error event: {tool_error_event}")
                            yield tool_error_event
                        
                        # è¾“å‡ºå·¥å…·ç»“æœ
                        tool_result_event = {
                            "type": "tool_result",
                            "content": f"å·¥å…·è°ƒç”¨å®Œæˆ" + (f"ï¼Œæ£€ç´¢åˆ° {len(result.split('---')) if '---' in result else 1} æ¡ç›¸å…³æ–‡çŒ®" if tool_name == "retrieval" else ""),
                            "result": result,
                            "timestamp": datetime.now().isoformat(),
                            "elapsed_time": f"{retrieval_elapsed:.2f}ç§’" if 'retrieval_elapsed' in locals() else None
                        }
                        logger.debug(f"Yielding tool_result event: {tool_result_event}")
                        yield tool_result_event
                        
                        # è®¾ç½®has_retrieval_toolå’Œresultå˜é‡
                        has_retrieval_tool = (tool_name == "retrieval")
                        if has_retrieval_tool:
                            all_retrieval_results = [result]
                        else:
                            all_retrieval_results = []
                    
                    else:
                        # å¤šä¸ªtool_callï¼šå¹¶å‘å¤„ç†
                        logger.info(f"ğŸš€ æ£€æµ‹åˆ° {len(tool_calls_raw)} ä¸ªå·¥å…·è°ƒç”¨ï¼Œå¼€å§‹å¹¶å‘æ‰§è¡Œ")
                        
                        # å‘é€å¹¶å‘å¼€å§‹äº‹ä»¶
                        concurrent_start_event = {
                            "type": "concurrent_tool_calls_start",
                            "content": f"å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(tool_calls_raw)} ä¸ªå·¥å…·è°ƒç”¨",
                            "timestamp": datetime.now().isoformat()
                        }
                        yield concurrent_start_event
                        
                        # å®šä¹‰å•ä¸ªå·¥å…·è°ƒç”¨çš„æ‰§è¡Œå‡½æ•°
                        def execute_single_tool(tool_call_idx, tool_call_raw, content_ref):
                            """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨"""
                            tool_call_raw = tool_call_raw.strip()
                            tool_result = {
                                "index": tool_call_idx,
                                "success": False,
                                "result": None,
                                "tool_name": None,
                                "error": None,
                                "elapsed_time": 0
                            }
                            
                            try:
                                start_time = time.time()
                                
                                if "python" in tool_call_raw.lower():
                                    # Pythonä»£ç æ‰§è¡Œ
                                    try:
                                        code_raw = content_ref.split('<tool_call>')[tool_call_idx + 1].split('</tool_call>')[0].split('<code>')[1].split('</code>')[0].strip()
                                        result = TOOL_MAP['PythonInterpreter'].call(code_raw)
                                        tool_result["success"] = True
                                        tool_result["result"] = result
                                        tool_result["tool_name"] = "PythonInterpreter"
                                    except Exception as e:
                                        tool_result["error"] = f"[Python Interpreter Error]: {str(e)}"
                                        tool_result["result"] = tool_result["error"]
                                else:
                                    # å…¶ä»–å·¥å…·è°ƒç”¨
                                    tool_call = json5.loads(tool_call_raw)
                                    tool_name = tool_call.get('name', '')
                                    tool_args = tool_call.get('arguments', {})
                                    tool_result["tool_name"] = tool_name
                                    tool_result["tool_args"] = tool_args  # ä¿å­˜tool_argsä»¥ä¾¿åç»­ä½¿ç”¨
                                    
                                    # å¦‚æœæ˜¯æ£€ç´¢å·¥å…·ï¼Œè‡ªåŠ¨è¡¥å……dataset_ids
                                    if tool_name == "retrieval":
                                        provided_dataset_ids = tool_args.get("dataset_ids", [])
                                        if not provided_dataset_ids or len(provided_dataset_ids) == 0:
                                            classification_result = classify_question_and_get_dataset_ids(self.user_prompt)
                                            auto_dataset_ids = classification_result["dataset_ids"]
                                            tool_args["dataset_ids"] = auto_dataset_ids
                                            logger.debug(f"ğŸ“‹ å·¥å…· {tool_call_idx + 1}: LLMæœªæä¾›dataset_idsï¼Œè‡ªåŠ¨åˆ¤æ–­ç§‘å®¤: {classification_result['departments']}")
                                    
                                    result = self.custom_call_tool(tool_name, tool_args)
                                    tool_result["success"] = True
                                    tool_result["result"] = result
                                
                                tool_result["elapsed_time"] = time.time() - start_time
                                
                            except Exception as e:
                                tool_result["error"] = str(e)
                                tool_result["result"] = f'å·¥å…·è°ƒç”¨é”™è¯¯ ({tool_call_idx + 1}/{len(tool_calls_raw)}): {str(e)}'
                            
                            return tool_result
                        
                        # ä½¿ç”¨ThreadPoolExecutorå¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                        all_retrieval_results = []
                        has_retrieval_tool = False
                        tool_results = {}  # å­˜å‚¨æ‰€æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼ŒæŒ‰ç´¢å¼•æ’åº
                        
                        with ThreadPoolExecutor(max_workers=min(len(tool_calls_raw), 5)) as executor:
                            # æäº¤æ‰€æœ‰ä»»åŠ¡
                            future_to_index = {
                                executor.submit(execute_single_tool, idx, tool_call_raw, content): idx
                                for idx, tool_call_raw in enumerate(tool_calls_raw)
                            }
                            
                            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœï¼ˆä¸ç­‰å¾…æ‰€æœ‰å®Œæˆï¼‰
                            for future in as_completed(future_to_index):
                                tool_call_idx = future_to_index[future]
                                
                                # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                                if cancelled["value"]:
                                    logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å¹¶å‘å·¥å…·è°ƒç”¨")
                                    break
                                
                                try:
                                    tool_result = future.result()
                                    tool_results[tool_call_idx] = tool_result
                                    
                                    tool_name = tool_result["tool_name"]
                                    result = tool_result["result"]
                                    elapsed_time = tool_result["elapsed_time"]
                                    
                                    # å‘é€å·¥å…·æ‰§è¡Œäº‹ä»¶
                                    tool_exec_event = {
                                        "type": "tool_execution",
                                        "content": f"å¹¶å‘è°ƒç”¨å·¥å…· {tool_name} ({tool_call_idx + 1}/{len(tool_calls_raw)}) å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’",
                                        "tool_name": tool_name,
                                        "tool_args": tool_result.get("tool_args", {}),
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    yield tool_exec_event
                                    
                                    # å¦‚æœæ˜¯æ£€ç´¢å·¥å…·ï¼Œè®°å½•å¹¶ç´¯ç§¯ç»“æœ
                                    if tool_name == "retrieval":
                                        has_retrieval_tool = True
                                        # ç¬¬ä¸€è½®å¤šç»„æ£€ç´¢æ—¶ï¼Œåªåœ¨ç¬¬ä¸€ä¸ªæ£€ç´¢è°ƒç”¨æ—¶å¢åŠ æ£€ç´¢è½®æ¬¡è®¡æ•°
                                        if tool_call_idx == 0:
                                            retrieval_round_num += 1
                                        
                                        search_keyword = tool_result.get("tool_args", {}).get("question", "")
                                        if search_keyword:
                                            previous_search_keywords.append(search_keyword)
                                            logger.info(f"ğŸ” å¹¶å‘æ£€ç´¢è°ƒç”¨ {tool_call_idx + 1}/{len(tool_calls_raw)} å®Œæˆï¼Œå…³é”®è¯: {search_keyword}")
                                        
                                        all_retrieval_results.append((tool_call_idx, result))
                                    
                                    # å‘é€å·¥å…·ç»“æœäº‹ä»¶
                                    tool_result_event = {
                                        "type": "tool_result",
                                        "content": f"å·¥å…·è°ƒç”¨ {tool_call_idx + 1}/{len(tool_calls_raw)} å®Œæˆ" + (f"ï¼Œæ£€ç´¢åˆ° {len(result.split('---')) if '---' in result else 1} æ¡ç›¸å…³æ–‡çŒ®" if tool_name == "retrieval" else ""),
                                        "result": result,
                                        "timestamp": datetime.now().isoformat(),
                                        "elapsed_time": f"{elapsed_time:.2f}ç§’"
                                    }
                                    yield tool_result_event
                                    
                                except Exception as e:
                                    logger.error(f"âŒ å·¥å…·è°ƒç”¨ {tool_call_idx + 1} æ‰§è¡Œå‡ºé”™: {str(e)}")
                                    error_event = {
                                        "type": "tool_error",
                                        "content": f"å·¥å…·è°ƒç”¨ {tool_call_idx + 1} å‡ºé”™: {str(e)}",
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    yield error_event
                        
                        # æŒ‰ç´¢å¼•é¡ºåºæ’åºæ£€ç´¢ç»“æœ
                        if has_retrieval_tool:
                            all_retrieval_results.sort(key=lambda x: x[0])
                            all_retrieval_results = [result for _, result in all_retrieval_results]
                        
                        # å‘é€å¹¶å‘å®Œæˆäº‹ä»¶
                        concurrent_complete_event = {
                            "type": "concurrent_tool_calls_complete",
                            "content": f"å¹¶å‘æ‰§è¡Œå®Œæˆï¼Œå…± {len(tool_calls_raw)} ä¸ªå·¥å…·è°ƒç”¨",
                            "timestamp": datetime.now().isoformat()
                        }
                        yield concurrent_complete_event
                    
                    # å¤„ç†æ£€ç´¢å·¥å…·çš„ç»“æœåˆå¹¶ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ£€ç´¢è°ƒç”¨ï¼‰
                    if has_retrieval_tool:
                        if len(all_retrieval_results) > 1:
                            # åˆå¹¶æ‰€æœ‰æ£€ç´¢ç»“æœ
                            combined_result = "\n\n--- æ£€ç´¢ç»“æœåˆ†å‰²çº¿ ---\n\n".join(all_retrieval_results)
                            result = combined_result
                            logger.info(f"âœ… åˆå¹¶äº† {len(all_retrieval_results)} ç»„æ£€ç´¢ç»“æœ")
                            
                            # å‘é€åˆå¹¶ç»“æœäº‹ä»¶
                            combined_result_event = {
                                "type": "retrieval_combined",
                                "content": f"å·²åˆå¹¶ {len(all_retrieval_results)} ç»„æ£€ç´¢ç»“æœ",
                                "result": result,
                                "timestamp": datetime.now().isoformat()
                            }
                            yield combined_result_event
                        elif len(all_retrieval_results) == 1:
                            result = all_retrieval_results[0]
                        else:
                            # æ²¡æœ‰æ£€ç´¢ç»“æœï¼Œä½¿ç”¨é»˜è®¤é”™è¯¯æ¶ˆæ¯
                            result = "[Retrieval] Error: No retrieval results"
                    
                    # å¦‚æœæ˜¯æ£€ç´¢å·¥å…·ï¼Œæµå¼åˆ¤æ–­ç»“æœæ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼ˆåªåœ¨æ‰€æœ‰æ£€ç´¢å®Œæˆååˆ¤æ–­ä¸€æ¬¡ï¼‰
                    if has_retrieval_tool and result and not result.startswith("[Retrieval] Error"):
                        judgment_start_time = time.time()
                        judgment_start_event = {
                            "type": "retrieval_judgment",
                            "content": "æ­£åœ¨è¯„ä¼°æ£€ç´¢å†…å®¹æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜...",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding retrieval_judgment event: {judgment_start_event}")
                        yield judgment_start_event
                        
                        try:
                            # ä½¿ç”¨æµå¼åˆ¤æ–­æ–¹æ³•ï¼Œå®æ—¶å‘é€åˆ¤æ–­æ–‡æœ¬
                            judgment = None
                            accumulated_judgment_text = ""
                            
                            for judgment_event in self.answer_system.judge_retrieval_sufficiency_stream(self.user_prompt, result):
                                event_type = judgment_event.get("type")
                                
                                if event_type == "judgment_chunk":
                                    # æµå¼å‘é€åˆ¤æ–­æ–‡æœ¬ç‰‡æ®µï¼ˆå¢é‡chunkï¼‰
                                    chunk_text = judgment_event.get("content", "")  # å¢é‡chunkå†…å®¹
                                    accumulated_judgment_text = judgment_event.get("accumulated", "")
                                    chunk_event = {
                                        "type": "judgment_streaming",
                                        "content": chunk_text,  # å‘é€å¢é‡chunkï¼Œè€Œä¸æ˜¯ç´¯ç§¯å†…å®¹
                                        "accumulated": accumulated_judgment_text,  # å¯é€‰ï¼šä¹Ÿæä¾›ç´¯ç§¯å†…å®¹ç»™å‰ç«¯
                                        "is_streaming": True,
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    yield chunk_event
                                    
                                elif event_type == "judgment_complete":
                                    # åˆ¤æ–­å®Œæˆï¼Œè·å–æœ€ç»ˆç»“æœ
                                    judgment = judgment_event.get("judgment", {})
                                    judgment_elapsed = time.time() - judgment_start_time
                                    logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ£€ç´¢ç»“æœè¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {judgment_elapsed:.2f} ç§’")
                                    logger.debug(f"Judgment complete: {judgment}")
                                    
                                    # å‘é€æµå¼å®Œæˆäº‹ä»¶ï¼ˆåœæ­¢å…‰æ ‡é—ªçƒï¼‰
                                    if accumulated_judgment_text:
                                        judgment_final_event = {
                                            "type": "judgment_streaming",
                                            "content": accumulated_judgment_text,
                                            "is_streaming": False,  # æ ‡è®°æµå¼ç»“æŸ
                                            "timestamp": datetime.now().isoformat(),
                                            "elapsed_time": f"{judgment_elapsed:.2f}ç§’"
                                        }
                                        yield judgment_final_event
                                    
                                elif event_type == "judgment_error":
                                    # åˆ¤æ–­å‡ºé”™ï¼Œè®°å½•ä½†ç»§ç»­
                                    logger.debug(f"Judgment error: {judgment_event.get('content')}")
                                    judgment = {"can_answer": True, "confidence": 0.5}  # é»˜è®¤å‡è®¾å¯ä»¥å›ç­”
                            
                            # å¦‚æœåˆ¤æ–­ç»“æœä¸ºç©ºï¼ˆå‡ºé”™ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            if judgment is None:
                                judgment = {"can_answer": True, "confidence": 0.5}
                            
                            # å¦‚æœæ£€ç´¢å†…å®¹è¶³å¤Ÿï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä¸ç»§ç»­æ¨ç†
                            if judgment.get('can_answer', False):
                                answer_start_time = time.time()
                                answer_generation_event = {
                                    "type": "answer_generation", 
                                    "content": f"æ£€ç´¢å†…å®¹å¯ä»¥å›ç­”é—®é¢˜ï¼ˆç½®ä¿¡åº¦: {judgment.get('confidence', 0):.2f}ï¼‰ï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...",
                                    "timestamp": datetime.now().isoformat()
                                }
                                logger.debug(f"Yielding answer_generation event: {answer_generation_event}")
                                yield answer_generation_event
                                
                                try:
                                    # è§£ææ£€ç´¢ç»“æœ
                                    retrieval_results = self.answer_system.parse_retrieval_results(result)
                                    
                                    # ä½¿ç”¨æµå¼ç”Ÿæˆç­”æ¡ˆ
                                    logger.debug(f"[DEBUG] Starting streaming answer generation...")
                                    
                                    accumulated_answer = ""
                                    answer_data = None
                                    first_chunk = True
                                    
                                    for stream_event in self.answer_system.generate_answer_with_citations_stream(self.user_prompt, retrieval_results):
                                        event_type = stream_event.get("type")
                                        
                                        if event_type == "answer_chunk":
                                            # é€å—å‘é€å¢é‡chunkï¼ˆä»…ç­”æ¡ˆä¸»ä½“ï¼Œä¸å«å‚è€ƒæ–‡çŒ®ï¼‰
                                            chunk_content = stream_event.get("content", "")  # å¢é‡chunkå†…å®¹
                                            accumulated_answer += chunk_content
                                            
                                            # ä½¿ç”¨final_answer_chunkç±»å‹ï¼Œå‰ç«¯ä¼šç”¨æœ€ç»ˆç­”æ¡ˆæ ·å¼æ¸²æŸ“
                                            chunk_event = {
                                                "type": "final_answer_chunk",
                                                "content": chunk_content,  # å‘é€å¢é‡chunkï¼Œè€Œä¸æ˜¯ç´¯ç§¯å†…å®¹
                                                "accumulated": accumulated_answer,  # å¯é€‰ï¼šä¹Ÿæä¾›ç´¯ç§¯å†…å®¹ç»™å‰ç«¯
                                                "is_streaming": True,  # æ ‡è®°ä¸ºæµå¼ä¸­
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            yield chunk_event
                                            
                                        elif event_type == "answer_complete":
                                            # ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œè·å–å®Œæ•´çš„ answer_dataï¼ˆåŒ…å« citationsï¼‰
                                            answer_data = stream_event.get("answer_data", {})
                                            logger.debug(f"[DEBUG] Answer streaming completed, citations count: {len(answer_data.get('citations', []))}")
                                            
                                            # ç›´æ¥ä½¿ç”¨ answer å­—æ®µå†…å®¹ï¼Œä¸è¿›è¡Œæ ¼å¼åŒ–
                                            # accumulated_answer å·²åŒ…å«æµå¼ä¼ è¾“çš„ç­”æ¡ˆä¸»ä½“
                                            final_answer_content = accumulated_answer.strip() if accumulated_answer else answer_data.get("answer", "")
                                            
                                            answer_elapsed = time.time() - answer_start_time
                                            logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {answer_elapsed:.2f} ç§’")
                                            
                                            # ç›´æ¥ä¼ é€’ answer_complete äº‹ä»¶ï¼Œè®©å‰ç«¯ç«‹å³æ˜¾ç¤ºå‚è€ƒæ–‡çŒ®
                                            answer_complete_event = {
                                                "type": "answer_complete",
                                                "content": final_answer_content,  # åªå‘é€ç­”æ¡ˆä¸»ä½“ï¼Œä¸å« "å‚è€ƒæ–‡çŒ®:" æ ¼å¼åŒ–
                                                "answer_data": answer_data,  # å‰ç«¯ä»è¿™é‡Œæå– citations
                                                "is_streaming": False,  # æ ‡è®°æµå¼ç»“æŸ
                                                "timestamp": datetime.now().isoformat(),
                                                "elapsed_time": f"{answer_elapsed:.2f}ç§’"
                                            }
                                            logger.debug(f"Yielding answer_complete event with citations (from retrieval stream)")
                                            yield answer_complete_event
                                            
                                        elif event_type == "answer_error":
                                            # ç­”æ¡ˆç”Ÿæˆå‡ºé”™ - ç«‹å³è¿”å›ï¼Œä¸å†å‘é€completedäº‹ä»¶
                                            error_content = stream_event.get("content", "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™")
                                            error_event = {
                                                "type": "error",
                                                "content": error_content,
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            logger.debug(f"Answer generation error: {error_content}")
                                            yield error_event
                                            
                                            # å‘é€completedäº‹ä»¶åç«‹å³è¿”å›
                                            completed_event = {
                                                "type": "completed",
                                                "content": "ç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œæµç¨‹ç»“æŸ",
                                    "timestamp": datetime.now().isoformat()
                                }
                                            logger.debug(f"Yielding completed event (after error): {completed_event}")
                                            yield completed_event
                                            return  # ç«‹å³è¿”å›ï¼Œé¿å…åç»­å¤„ç†
                                    
                                    # å‘é€å®Œæˆäº‹ä»¶ï¼ˆæ­£å¸¸æµç¨‹ï¼‰
                                    total_elapsed = time.time() - start_time
                                    logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ•´ä¸ªæµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.2f} ç§’")
                                    
                                    completed_event = {
                                        "type": "completed",
                                        "content": "åŸºäºæ£€ç´¢å†…å®¹ç”Ÿæˆç­”æ¡ˆå®Œæˆ",
                                        "timestamp": datetime.now().isoformat(),
                                        "total_elapsed_time": f"{total_elapsed:.2f}ç§’"
                                    }
                                    logger.debug(f"Yielding completed event (from retrieval): {completed_event}")
                                    yield completed_event
                                    logger.info(f"=== StreamingReactAgent.stream_run COMPLETED (FROM RETRIEVAL) ===")
                                    return
                                    
                                except Exception as e:
                                    error_event = {
                                        "type": "error",
                                        "content": f"ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    logger.debug(f"Error generating final answer from retrieval: {str(e)}")
                                    import traceback
                                    traceback.print_exc()
                                    logger.debug(f"Yielding error event: {error_event}")
                                    yield error_event
                                    
                                    completed_event = {
                                        "type": "completed",
                                        "content": "ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œæµç¨‹ç»“æŸ",
                                    "timestamp": datetime.now().isoformat()
                                }
                                    logger.debug(f"Yielding completed event (error): {completed_event}")
                                yield completed_event
                                return
                            else:
                                # æ£€ç´¢å†…å®¹ä¸è¶³ï¼Œè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ï¼ˆæ€è€ƒâ†’æ£€ç´¢â†’åˆ¤æ–­ï¼‰
                                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡
                                if retrieval_round_num >= MAX_RETRIEVAL_ROUNDS:
                                    # å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡ï¼Œè¿”å›æŠ±æ­‰æ¶ˆæ¯
                                    logger.warning(f"âš ï¸ ç¬¬ {retrieval_round_num} è½®æ£€ç´¢åä»æ— æ³•å›ç­”é—®é¢˜ï¼Œå·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡ {MAX_RETRIEVAL_ROUNDS}")
                                    
                                    no_answer_event = {
                                        "type": "final_answer",
                                        "content": "æŠ±æ­‰ï¼Œç»è¿‡å¤šè½®æ£€ç´¢åï¼Œæˆ‘æ— æ³•åŸºäºç°æœ‰çŸ¥è¯†åº“æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨ï¼š\n\n1. å°è¯•ä½¿ç”¨æ›´å…·ä½“æˆ–ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n2. å°†é—®é¢˜æ‹†åˆ†ä¸ºæ›´å°çš„å­é—®é¢˜\n3. å¦‚æœå¯èƒ½ï¼Œæä¾›æ›´å¤šç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯",
                                        "timestamp": datetime.now().isoformat(),
                                        "is_streaming": False
                                    }
                                    logger.debug(f"Yielding no_answer event: {no_answer_event}")
                                    yield no_answer_event
                                    
                                    completed_event = {
                                        "type": "completed",
                                        "content": f"å·²è¾¾åˆ°æœ€å¤§æ£€ç´¢è½®æ¬¡ï¼ˆ{MAX_RETRIEVAL_ROUNDS}æ¬¡ï¼‰ï¼Œæœªèƒ½æ‰¾åˆ°ç­”æ¡ˆ",
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    logger.debug(f"Yielding completed event (max rounds): {completed_event}")
                                    yield completed_event
                                    logger.info(f"=== StreamingReactAgent.stream_run MAX_ROUNDS_REACHED ===")
                                    return
                                
                                continue_reasoning_event = {
                                    "type": "continue_reasoning",
                                    "content": f"æ£€ç´¢å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼ˆç½®ä¿¡åº¦: {judgment.get('confidence', 0):.2f}ï¼‰ï¼Œç»§ç»­ä¸‹ä¸€è½®æ£€ç´¢ï¼ˆç¬¬ {retrieval_round_num + 1}/{MAX_RETRIEVAL_ROUNDS} è½®ï¼‰...",
                                    "timestamp": datetime.now().isoformat()
                                }
                                logger.debug(f"Yielding continue_reasoning event: {continue_reasoning_event}")
                                yield continue_reasoning_event
                                
                                # åˆ¤æ–­ä¸ºä¸èƒ½å›ç­”æ—¶ï¼Œä¸æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€è½®å¾ªç¯
                                # è¿™æ ·ä¸‹ä¸€è½®ä¼šé‡æ–°æ€è€ƒæ£€ç´¢ç­–ç•¥ï¼Œä½¿ç”¨ä¸åŒçš„å…³é”®è¯
                                logger.info(f"ğŸ”„ æ£€ç´¢ç»“æœä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼ˆç½®ä¿¡åº¦: {judgment.get('confidence', 0):.2f}ï¼‰ï¼Œè·³è¿‡æ·»åŠ å·¥å…·ç»“æœï¼Œè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ï¼ˆæ€è€ƒâ†’æ£€ç´¢â†’åˆ¤æ–­ï¼‰")
                                logger.info(f"ğŸ“ å·²ä½¿ç”¨è¿‡çš„æ£€ç´¢å…³é”®è¯: {previous_search_keywords}")
                                continue  # ç›´æ¥è·³åˆ°ä¸‹ä¸€è½®å¾ªç¯
                                
                        except Exception as e:
                            judgment_error_event = {
                                "type": "judgment_error",
                                "content": f"æ£€ç´¢å†…å®¹è¯„ä¼°å‡ºé”™: {str(e)}",
                                "timestamp": datetime.now().isoformat()
                            }
                            logger.debug(f"Yielding judgment_error event: {judgment_error_event}")
                            yield judgment_error_event
                            # è¯„ä¼°å‡ºé”™æ—¶ï¼Œä¹Ÿè·³è¿‡æ·»åŠ å·¥å…·ç»“æœï¼Œè¿›å…¥ä¸‹ä¸€è½®
                            logger.info(f"ğŸ”„ æ£€ç´¢ç»“æœè¯„ä¼°å‡ºé”™ï¼Œè·³è¿‡æ·»åŠ å·¥å…·ç»“æœï¼Œè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯")
                            continue  # ç›´æ¥è·³åˆ°ä¸‹ä¸€è½®å¾ªç¯
                    
                    # åªæœ‰åœ¨éæ£€ç´¢å·¥å…·ï¼Œæˆ–è€…æ£€ç´¢å·¥å…·ä½†åˆ¤æ–­ç»“æœæœªæ˜ç¡®ä¸º"ä¸èƒ½å›ç­”"æ—¶ï¼Œæ‰æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                    # å¯¹äºæ£€ç´¢å·¥å…·ä¸”å·²åˆ¤æ–­ä¸º"ä¸èƒ½å›ç­”"çš„æƒ…å†µï¼Œå·²ç»åœ¨ä¸Šé¢continueäº†
                    result = "<tool_response>\n" + result + "\n</tool_response>"
                    messages.append({"role": "user", "content": result})
                    logger.debug(f"Added tool result to conversation")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
                if '<answer>' in content and '</answer>' in content:
                    answer = content.split('<answer>')[1].split('</answer>')[0]
                    final_answer_event = {
                        "type": "final_answer",
                        "content": answer.strip(),
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Yielding final_answer event: {final_answer_event}")
                    yield final_answer_event
                    
                    completed_event = {
                        "type": "completed",
                        "content": "æ¨ç†å®Œæˆ",
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Yielding completed event: {completed_event}")
                    yield completed_event
                    logger.info(f"=== StreamingReactAgent.stream_run COMPLETED ===")
                    return
                    
            except Exception as e:
                error_event = {
                    "type": "error",
                    "content": f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Exception in stream_run: {str(e)}")
                logger.debug(f"Yielding error event: {error_event}")
                yield error_event
                
                completed_event = {
                    "type": "completed",
                    "content": "æ¨ç†é”™è¯¯ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (error): {completed_event}")
                yield completed_event
                logger.info(f"=== StreamingReactAgent.stream_run ERROR ===")
                return
            
            # æ£€æŸ¥tokené™åˆ¶
            max_tokens = 108 * 1024
            token_count = self.count_tokens(messages)
            logger.debug(f"Current token count: {token_count}/{max_tokens}")
            
            if token_count > max_tokens:
                token_limit_event = {
                    "type": "token_limit",
                    "content": f"è¾¾åˆ°tokené™åˆ¶ ({token_count} > {max_tokens})ï¼Œå°è¯•ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding token_limit event: {token_limit_event}")
                yield token_limit_event
                
                messages[-1]['content'] = "You have now reached the maximum context length you can handle. You should stop making tool calls and, based on all the information above, think again and provide what you consider the most likely answer in the following format:<think>your final thinking</think>\n<answer>your answer</answer>"
                
                try:
                    content = self.call_server(messages, planning_port)
                    messages.append({"role": "assistant", "content": content.strip()})
                    
                    if '<answer>' in content and '</answer>' in content:
                        answer = content.split('<answer>')[1].split('</answer>')[0]
                        final_answer_event = {
                            "type": "final_answer",
                            "content": answer.strip(),
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding final_answer event (token limit): {final_answer_event}")
                        yield final_answer_event
                    else:
                        final_answer_event = {
                            "type": "final_answer",
                            "content": content.strip(),
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding final_answer event (token limit, no format): {final_answer_event}")
                        yield final_answer_event
                except Exception as e:
                    error_event = {
                        "type": "error",
                        "content": f"ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Error generating final answer: {str(e)}")
                    logger.debug(f"Yielding error event: {error_event}")
                    yield error_event
                
                # å‘é€ completed äº‹ä»¶ï¼ˆæ— è®ºæˆåŠŸä¸å¦ï¼‰
                completed_event = {
                    "type": "completed",
                    "content": "Tokené™åˆ¶ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (token limit): {completed_event}")
                yield completed_event
                logger.info(f"=== StreamingReactAgent.stream_run TOKEN_LIMIT_END ===")
                return
                
            round_end_event = {
                "type": "round_end",
                "content": f"ç¬¬ {round_num} è½®æ¨ç†ç»“æŸ",
                "round": round_num,
                "timestamp": datetime.now().isoformat()
            }
            logger.debug(f"Yielding round_end event: {round_end_event}")
            yield round_end_event
        
        # å¦‚æœå¾ªç¯ç»“æŸä»æœªæ‰¾åˆ°ç­”æ¡ˆ
        no_answer_event = {
            "type": "no_answer",
            "content": "æœªæ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ¨ç†è½®æ¬¡",
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Yielding no_answer event: {no_answer_event}")
        yield no_answer_event
        
        # å‘é€ completed äº‹ä»¶
        completed_event = {
            "type": "completed",
            "content": "æ¨ç†å®Œæˆï¼ˆæœªæ‰¾åˆ°ç­”æ¡ˆï¼‰",
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Yielding completed event (no answer): {completed_event}")
        yield completed_event
        logger.info(f"=== StreamingReactAgent.stream_run NO_ANSWER_END ===")



