import json
import json5
import os
import time
import asyncio
import logging
from typing import Dict, Iterator, List, Optional, Generator
from datetime import datetime
from react_agent import MultiTurnReactAgent, TOOL_MAP, MAX_LLM_CALL_PER_RUN, today_date
from prompt import SYSTEM_PROMPT
from answer_system import AnswerJudgmentSystem

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class StreamingReactAgent(MultiTurnReactAgent):
    """æµå¼æ¨ç†ä»£ç†ï¼Œæ”¯æŒå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹å’Œå·¥å…·è°ƒç”¨"""
    
    def __init__(self, llm=None, function_list=None, **kwargs):
        logger.debug(f"Initializing StreamingReactAgent with llm: {llm}, function_list: {function_list}")
        
        if llm is None:
            raise ValueError("llm parameter is required")
        
        self.llm_generate_cfg = llm["generate_cfg"]
        self.llm_model = llm["model"]
        self.llm_base_url = llm.get("base_url", "http://127.0.0.1:6001/v1")
        self.llm_api_key = llm.get("api_key", "EMPTY")
        
        # ä¸è°ƒç”¨çˆ¶ç±»çš„__init__ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å…¶ä¸­çš„éƒ¨åˆ†åŠŸèƒ½
        self.function_list = function_list or []
        
        # åˆå§‹åŒ–ç­”æ¡ˆåˆ¤æ–­ç³»ç»Ÿ
        self.answer_system = AnswerJudgmentSystem()
        
    def call_server(self, msgs, max_tries=3):
        """è°ƒç”¨LLMæœåŠ¡å™¨"""
        from openai import OpenAI, APIError, APIConnectionError, APITimeoutError
        import random

        client = OpenAI(
            api_key=self.llm_api_key,
            base_url=self.llm_base_url,
            timeout=600.0,
        )

        base_sleep_time = 1 
        
        for attempt in range(max_tries):
            try:
                logger.debug(f"--- Attempting to call the service, try {attempt + 1}/{max_tries} ---")
                chat_response = client.chat.completions.create(
                    model=self.llm_model,
                    messages=msgs,
                    stop=["\n<tool_response>", "<tool_response>"],
                    temperature=self.llm_generate_cfg.get('temperature', 0.6),
                    top_p=self.llm_generate_cfg.get('top_p', 0.95),
                    logprobs=True,
                    max_tokens=8000,
                    presence_penalty=self.llm_generate_cfg.get('presence_penalty', 1.1)
                )
                content = chat_response.choices[0].message.content
                if content and content.strip():
                    logger.debug("--- Service call successful, received a valid response ---")
                    return content.strip()
                else:
                    logger.debug(f"Warning: Attempt {attempt + 1} received an empty response.")

            except (APIError, APIConnectionError, APITimeoutError) as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an API or network error: {e}")
            except Exception as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an unexpected error: {e}")

            if attempt < max_tries - 1:
                sleep_time = base_sleep_time * (2 ** attempt) + random.uniform(0, 1)
                sleep_time = min(sleep_time, 30) 
                
                logger.debug(f"Retrying in {sleep_time:.2f} seconds...")
                time.sleep(sleep_time)
            else:
                logger.debug("Error: All retry attempts have been exhausted. The call has failed.")
        
        return f"vllm server error!!!"
    
    def call_server_stream(self, msgs, max_tries=3, enable_thinking=True):
        """
        æµå¼è°ƒç”¨LLMæœåŠ¡å™¨ï¼Œæ”¯æŒé˜¿é‡Œäº‘æ¨¡å‹çš„thinkingæ¨¡å¼
        
        Args:
            msgs: æ¶ˆæ¯åˆ—è¡¨
            max_tries: æœ€å¤§é‡è¯•æ¬¡æ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆé˜¿é‡Œäº‘æ¨¡å‹ä¸“ç”¨ï¼‰
            
        Yields:
            dict: åŒ…å« type å’Œ content çš„å­—å…¸
                - type: 'reasoning' (æ€è€ƒè¿‡ç¨‹) æˆ– 'content' (å›ç­”å†…å®¹)
                - content: æ–‡æœ¬å†…å®¹
        """
        from openai import OpenAI, APIError, APIConnectionError, APITimeoutError
        import random

        client = OpenAI(
            api_key=self.llm_api_key,
            base_url=self.llm_base_url,
            timeout=600.0,
        )

        base_sleep_time = 1 
        
        for attempt in range(max_tries):
            try:
                logger.debug(f"--- Attempting to call the service (stream), try {attempt + 1}/{max_tries} ---")
                
                # æ„å»ºAPIè°ƒç”¨å‚æ•°
                api_params = {
                    "model": self.llm_model,
                    "messages": msgs,
                    "stop": ["\n<tool_response>", "<tool_response>"],
                    "temperature": self.llm_generate_cfg.get('temperature', 0.6),
                    "top_p": self.llm_generate_cfg.get('top_p', 0.95),
                    "max_tokens": 1000,
                    "presence_penalty": self.llm_generate_cfg.get('presence_penalty', 1.1),
                    "stream": True
                }
                
                # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œæ·»åŠ extra_bodyå‚æ•°ï¼ˆé˜¿é‡Œäº‘æ¨¡å‹ä¸“ç”¨ï¼‰
                if enable_thinking and 'qwen' in self.llm_model.lower():
                    api_params["extra_body"] = {
                        "enable_thinking": True,
                        "thinking_budget": 1000  # æœ€å¤§æ€è€ƒtokenæ•°
                    }
                    logger.info(f"ğŸ§  å·²å¯ç”¨æ€è€ƒæ¨¡å¼ (thinking_budget=1000)")
                
                stream = client.chat.completions.create(**api_params)
                
                accumulated_reasoning = ""
                accumulated_content = ""
                has_reasoning = False
                has_content = False
                
                for chunk in stream:
                    if not chunk.choices:
                        continue
                    
                    delta = chunk.choices[0].delta
                    
                    # å¤„ç†æ€è€ƒå†…å®¹ (reasoning_content)
                    if hasattr(delta, "reasoning_content") and delta.reasoning_content is not None:
                        has_reasoning = True
                        accumulated_reasoning += delta.reasoning_content
                        yield {
                            "type": "reasoning",
                            "content": delta.reasoning_content,
                            "accumulated": accumulated_reasoning
                        }
                    
                    # å¤„ç†å›ç­”å†…å®¹ (content)
                    if hasattr(delta, "content") and delta.content:
                        has_content = True
                        accumulated_content += delta.content
                        yield {
                            "type": "content",
                            "content": delta.content,
                            "accumulated": accumulated_content
                        }
                
                if has_reasoning or has_content:
                    logger.info(f"âœ… æµå¼è°ƒç”¨æˆåŠŸ - æ€è€ƒ: {len(accumulated_reasoning)}å­—, å›ç­”: {len(accumulated_content)}å­—")
                    return  # æˆåŠŸå®Œæˆ
                else:
                    logger.debug(f"Warning: Attempt {attempt + 1} received an empty response.")

            except (APIError, APIConnectionError, APITimeoutError) as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an API or network error: {e}")
            except Exception as e:
                logger.debug(f"Error: Attempt {attempt + 1} failed with an unexpected error: {e}")

            if attempt < max_tries - 1:
                sleep_time = base_sleep_time * (2 ** attempt) + random.uniform(0, 1)
                sleep_time = min(sleep_time, 30) 
                
                logger.debug(f"Retrying in {sleep_time:.2f} seconds...")
                time.sleep(sleep_time)
            else:
                logger.debug("Error: All retry attempts have been exhausted. The call has failed.")
        
        yield {"type": "error", "content": "vllm server error!!!"}
        
    def count_tokens(self, messages, model="gpt-4o"):
        """è®¡ç®—tokenæ•°é‡"""
        try: 
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.llm_model) 
        except Exception as e: 
            import tiktoken
            tokenizer = tiktoken.encoding_for_model(model)
        
        from qwen_agent.llm.schema import Message
        from qwen_agent.utils.utils import build_text_completion_prompt
        
        full_message = [Message(**x) for x in messages]
        full_prompt = build_text_completion_prompt(full_message, allow_special=True)
        
        return len(tokenizer.encode(full_prompt))
        
    def custom_call_tool(self, tool_name: str, tool_args: dict, **kwargs):
        """è°ƒç”¨å·¥å…·"""
        logger.debug(f"[DEBUG] custom_call_tool called with: tool_name={tool_name}, tool_args={tool_args}")
        
        if tool_name in TOOL_MAP:
            if "python" in tool_name.lower():
                result = TOOL_MAP['PythonInterpreter'].call(tool_args)
            elif tool_name == "parse_file":
                params = {"files": tool_args["files"]}
                
                raw_result = asyncio.run(TOOL_MAP[tool_name].call(params, file_root_path="./eval_data/file_corpus"))
                result = raw_result

                if not isinstance(raw_result, str):
                    result = str(raw_result)
            else:
                # ç›´æ¥ä¼ é€’tool_argsï¼Œä¸è¦æ·»åŠ é¢å¤–çš„paramsåŒ…è£…
                raw_result = TOOL_MAP[tool_name].call(tool_args, **kwargs)
                result = raw_result
            return result

        else:
            return f"Error: Tool {tool_name} not found"
        
    def stream_run(self, question: str, cancelled: dict = None) -> Generator[Dict, None, None]:
        """
        æµå¼è¿è¡Œæ¨ç†è¿‡ç¨‹ï¼Œå®æ—¶è¾“å‡ºå„ä¸ªé˜¶æ®µçš„ä¿¡æ¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            planning_port: vLLMæœåŠ¡å™¨ç«¯å£
            cancelled: å–æ¶ˆæ ‡è®°å­—å…¸ {"value": False}ï¼Œå½“è®¾ç½®ä¸º True æ—¶ä¸­æ–­å¤„ç†
            
        Yields:
            Dict: åŒ…å«å½“å‰é˜¶æ®µä¿¡æ¯çš„å­—å…¸
        """
        # åˆå§‹åŒ– cancelled æ ‡è®°
        if cancelled is None:
            cancelled = {"value": False}
        logger.info(f"=== StreamingReactAgent.stream_run START ===")
        logger.debug(f"Question: {question}")
        logger.debug(f"LLM Model: {self.llm_model}")
        
        start_time = time.time()
        self.user_prompt = question
        
        # åˆå§‹åŒ–
        system_prompt = SYSTEM_PROMPT + str(today_date())
        messages = [
            {"role": "system", "content": system_prompt}, 
            {"role": "user", "content": question}
        ]
        
        init_event = {
            "type": "init",
            "content": f"å¼€å§‹å¤„ç†é—®é¢˜...",
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Yielding init event: {init_event}")
        yield init_event
        
        num_llm_calls_available = MAX_LLM_CALL_PER_RUN
        round_num = 0
        
        logger.debug(f"Starting main loop, max calls: {MAX_LLM_CALL_PER_RUN}")
        
        while num_llm_calls_available > 0:
            # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
            if cancelled["value"]:
                cancelled_event = {
                    "type": "cancelled",
                    "content": "æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å¤„ç†",
                    "timestamp": datetime.now().isoformat()
                }
                logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢æ¨ç†å¾ªç¯")
                yield cancelled_event
                
                completed_event = {
                    "type": "completed",
                    "content": "å®¢æˆ·ç«¯æ–­å¼€ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (cancelled): {completed_event}")
                yield completed_event
                return
            
            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > 150 * 60:  # 150åˆ†é’Ÿ
                timeout_event = {
                    "type": "timeout",
                    "content": "æ¨ç†è¶…æ—¶ï¼ˆ150åˆ†é’Ÿï¼‰",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding timeout event: {timeout_event}")
                yield timeout_event
                
                completed_event = {
                    "type": "completed",
                    "content": "æ¨ç†è¶…æ—¶ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (timeout): {completed_event}")
                yield completed_event
                return
                
            round_num += 1
            num_llm_calls_available -= 1
            
            round_start_event = {
                "type": "round_start",
                "content": f"ç¬¬ {round_num} è½®æ¨ç†å¼€å§‹",
                "round": round_num,
                "timestamp": datetime.now().isoformat()
            }
            logger.debug(f"Yielding round_start event: {round_start_event}")
            yield round_start_event
            
            # è°ƒç”¨LLM
            thinking_start_time = time.time()
            thinking_start_event = {
                "type": "thinking_start",
                "content": "æ­£åœ¨æ€è€ƒ...",
                "timestamp": datetime.now().isoformat()
            }
            logger.debug(f"Yielding thinking_start event: {thinking_start_event}")
            yield thinking_start_event
            
            try:
                logger.debug(f"Calling LLM server (stream) - Model: {self.llm_model}")
                
                # ä½¿ç”¨æ–°çš„æµå¼APIï¼Œæ”¯æŒé˜¿é‡Œäº‘thinkingæ¨¡å¼
                reasoning_content = ""  # å®Œæ•´æ€è€ƒè¿‡ç¨‹
                answer_content = ""  # å®Œæ•´å›ç­”å†…å®¹
                is_answering = False  # æ˜¯å¦è¿›å…¥å›ç­”é˜¶æ®µ
                
                for chunk_data in self.call_server_stream(messages, enable_thinking=True):
                    # åœ¨æµå¼æ¥æ”¶è¿‡ç¨‹ä¸­æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                    if cancelled["value"]:
                        logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢LLMæµå¼æ¥æ”¶")
                        return
                    
                    chunk_type = chunk_data.get("type")
                    chunk_content = chunk_data.get("content", "")
                    
                    # å¤„ç†æ€è€ƒå†…å®¹ï¼ˆreasoningï¼‰
                    if chunk_type == "reasoning":
                        reasoning_content = chunk_data.get("accumulated", reasoning_content)
                        # å®æ—¶å‘é€æ€è€ƒç‰‡æ®µ
                        if chunk_content.strip():
                            thinking_chunk_event = {
                                "type": "thinking_chunk",
                                "content": chunk_content,
                                "accumulated": reasoning_content,
                                "is_streaming": True,
                                "is_complete": False,
                                "timestamp": datetime.now().isoformat()
                            }
                            yield thinking_chunk_event
                    
                    # å¤„ç†å›ç­”å†…å®¹ï¼ˆcontentï¼‰
                    elif chunk_type == "content":
                        if not is_answering:
                            # ç¬¬ä¸€æ¬¡æ”¶åˆ°contentï¼Œè¯´æ˜æ€è€ƒé˜¶æ®µç»“æŸ
                            is_answering = True
                            
                            # å¦‚æœæœ‰æ€è€ƒå†…å®¹ï¼Œå‘é€æ€è€ƒå®Œæˆäº‹ä»¶
                            if reasoning_content.strip():
                                thinking_elapsed = time.time() - thinking_start_time
                                logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ€è€ƒè¿‡ç¨‹å®Œæˆï¼Œè€—æ—¶: {thinking_elapsed:.2f} ç§’")
                                
                                thinking_complete_event = {
                                    "type": "thinking",
                                    "content": reasoning_content.strip(),
                                    "is_streaming": False,
                                    "timestamp": datetime.now().isoformat(),
                                    "elapsed_time": f"{thinking_elapsed:.2f}ç§’"
                                }
                                # logger.info(f"ğŸ’­ æ€è€ƒå†…å®¹é•¿åº¦: {len(reasoning_content)} å­—")
                                yield thinking_complete_event
                            else:
                                logger.info(f"âš ï¸ æ¨¡å‹æœªè¾“å‡ºæ€è€ƒå†…å®¹ï¼ˆå¯èƒ½æ˜¯éthinkingæ¨¡å‹ï¼‰")
                        
                        # ç´¯ç§¯å›ç­”å†…å®¹
                        answer_content += chunk_content
                    
                    # å¤„ç†é”™è¯¯
                    elif chunk_type == "error":
                        raise Exception(chunk_content)
                
                # ä½¿ç”¨answer_contentä½œä¸ºæœ€ç»ˆcontent
                content = answer_content if answer_content else reasoning_content
                # logger.info(f"âœ… LLMå“åº”å®Œæˆ - æ€è€ƒ: {len(reasoning_content)}å­—, å›ç­”: {len(answer_content)}å­—")
                
                # æ¸…ç†tool_responseæ ‡è®°
                if '<tool_response>' in content:
                    pos = content.find('<tool_response>')
                    content = content[:pos]
                    
                messages.append({"role": "assistant", "content": content.strip()})
                logger.debug(f"Added assistant message to conversation")
                
                # æ£€æŸ¥å·¥å…·è°ƒç”¨
                if '<tool_call>' in content and '</tool_call>' in content:
                    # åœ¨æ‰§è¡Œå·¥å…·è°ƒç”¨å‰æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                    if cancelled["value"]:
                        cancelled_event = {
                            "type": "cancelled",
                            "content": "æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å¤„ç†",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œè·³è¿‡å·¥å…·è°ƒç”¨")
                        yield cancelled_event
                        
                        completed_event = {
                            "type": "completed",
                            "content": "å®¢æˆ·ç«¯æ–­å¼€ï¼Œæµç¨‹ç»“æŸ",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding completed event (cancelled before tool): {completed_event}")
                        yield completed_event
                        return
                    
                    logger.debug(f"Found tool call in response")
                    tool_call_raw = content.split('<tool_call>')[1].split('</tool_call>')[0]
                    
                    tool_call_start_event = {
                        "type": "tool_call_start",
                        "content": f"å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call_raw[:100]}...",
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Yielding tool_call_start event: {tool_call_start_event}")
                    yield tool_call_start_event
                    
                    try:
                        if "python" in tool_call_raw.lower():
                            # Pythonä»£ç æ‰§è¡Œ
                            try:
                                code_raw = content.split('<tool_call>')[1].split('</tool_call>')[0].split('<code>')[1].split('</code>')[0].strip()
                                python_exec_event = {
                                    "type": "python_execution",
                                    "content": f"æ‰§è¡ŒPythonä»£ç :\n```python\n{code_raw}\n```",
                                    "code": code_raw,
                                    "timestamp": datetime.now().isoformat()
                                }
                                logger.debug(f"Yielding python_execution event: {python_exec_event}")
                                yield python_exec_event
                                
                                result = TOOL_MAP['PythonInterpreter'].call(code_raw)
                                logger.debug(f"Python execution result: {result[:200]}...")
                            except Exception as e:
                                result = f"[Python Interpreter Error]: {str(e)}"
                                logger.debug(f"Python execution error: {result}")
                        else:
                            # å…¶ä»–å·¥å…·è°ƒç”¨
                            tool_call = json5.loads(tool_call_raw)
                            tool_name = tool_call.get('name', '')
                            tool_args = tool_call.get('arguments', {})
                            
                            tool_exec_event = {
                                "type": "tool_execution",
                                "content": f"è°ƒç”¨å·¥å…· {tool_name}ï¼Œå‚æ•°: {json.dumps(tool_args, indent=2, ensure_ascii=False)}",
                                "tool_name": tool_name,
                                "tool_args": tool_args,
                                "timestamp": datetime.now().isoformat()
                            }
                            logger.debug(f"Yielding tool_execution event: {tool_exec_event}")
                            yield tool_exec_event
                            
                            retrieval_start_time = time.time()
                            logger.debug(f"Calling tool {tool_name} with args {tool_args}")
                            result = self.custom_call_tool(tool_name, tool_args)
                            retrieval_elapsed = time.time() - retrieval_start_time
                            logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ£€ç´¢å·¥å…·æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {retrieval_elapsed:.2f} ç§’")
                            logger.debug(f"Tool result: {result[:200]}...")
                            
                            # å·¥å…·è°ƒç”¨å®Œæˆåæ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                            if cancelled["value"]:
                                logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œåœæ­¢å·¥å…·ç»“æœå¤„ç†")
                                return
                            
                    except Exception as e:
                        result = f'å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}'
                        tool_error_event = {
                            "type": "tool_error",
                            "content": result,
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding tool_error event: {tool_error_event}")
                        yield tool_error_event
                    
                    # è¾“å‡ºå·¥å…·ç»“æœ
                    tool_result_event = {
                        "type": "tool_result",
                        "content": f"æ£€ç´¢åˆ° {len(result.split('---')) if '---' in result else 1} æ¡ç›¸å…³æ–‡çŒ®",
                        "result": result,
                        "timestamp": datetime.now().isoformat(),
                        "elapsed_time": f"{retrieval_elapsed:.2f}ç§’" if 'retrieval_elapsed' in locals() else None
                    }
                    logger.debug(f"Yielding tool_result event: {tool_result_event}")
                    yield tool_result_event
                    
                    # å¦‚æœæ˜¯æ£€ç´¢å·¥å…·ï¼Œæµå¼åˆ¤æ–­ç»“æœæ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜
                    if tool_name == "retrieval" and result and not result.startswith("[Retrieval] Error"):
                        judgment_start_time = time.time()
                        judgment_start_event = {
                            "type": "retrieval_judgment",
                            "content": "æ­£åœ¨è¯„ä¼°æ£€ç´¢å†…å®¹æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜...",
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding retrieval_judgment event: {judgment_start_event}")
                        yield judgment_start_event
                        
                        try:
                            # ä½¿ç”¨æµå¼åˆ¤æ–­æ–¹æ³•ï¼Œå®æ—¶å‘é€åˆ¤æ–­æ–‡æœ¬
                            judgment = None
                            accumulated_judgment_text = ""
                            
                            for judgment_event in self.answer_system.judge_retrieval_sufficiency_stream(self.user_prompt, result):
                                event_type = judgment_event.get("type")
                                
                                if event_type == "judgment_chunk":
                                    # æµå¼å‘é€åˆ¤æ–­æ–‡æœ¬ç‰‡æ®µï¼ˆå¢é‡chunkï¼‰
                                    chunk_text = judgment_event.get("content", "")  # å¢é‡chunkå†…å®¹
                                    accumulated_judgment_text = judgment_event.get("accumulated", "")
                                    chunk_event = {
                                        "type": "judgment_streaming",
                                        "content": chunk_text,  # å‘é€å¢é‡chunkï¼Œè€Œä¸æ˜¯ç´¯ç§¯å†…å®¹
                                        "accumulated": accumulated_judgment_text,  # å¯é€‰ï¼šä¹Ÿæä¾›ç´¯ç§¯å†…å®¹ç»™å‰ç«¯
                                        "is_streaming": True,
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    yield chunk_event
                                    
                                elif event_type == "judgment_complete":
                                    # åˆ¤æ–­å®Œæˆï¼Œè·å–æœ€ç»ˆç»“æœ
                                    judgment = judgment_event.get("judgment", {})
                                    judgment_elapsed = time.time() - judgment_start_time
                                    logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ£€ç´¢ç»“æœè¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {judgment_elapsed:.2f} ç§’")
                                    logger.debug(f"Judgment complete: {judgment}")
                                    
                                    # å‘é€æµå¼å®Œæˆäº‹ä»¶ï¼ˆåœæ­¢å…‰æ ‡é—ªçƒï¼‰
                                    if accumulated_judgment_text:
                                        judgment_final_event = {
                                            "type": "judgment_streaming",
                                            "content": accumulated_judgment_text,
                                            "is_streaming": False,  # æ ‡è®°æµå¼ç»“æŸ
                                            "timestamp": datetime.now().isoformat(),
                                            "elapsed_time": f"{judgment_elapsed:.2f}ç§’"
                                        }
                                        yield judgment_final_event
                                    
                                elif event_type == "judgment_error":
                                    # åˆ¤æ–­å‡ºé”™ï¼Œè®°å½•ä½†ç»§ç»­
                                    logger.debug(f"Judgment error: {judgment_event.get('content')}")
                                    judgment = {"can_answer": True, "confidence": 0.5}  # é»˜è®¤å‡è®¾å¯ä»¥å›ç­”
                            
                            # å¦‚æœåˆ¤æ–­ç»“æœä¸ºç©ºï¼ˆå‡ºé”™ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            if judgment is None:
                                judgment = {"can_answer": True, "confidence": 0.5}
                            
                            # å¦‚æœæ£€ç´¢å†…å®¹è¶³å¤Ÿï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä¸ç»§ç»­æ¨ç†
                            if judgment.get('can_answer', False):
                                answer_start_time = time.time()
                                answer_generation_event = {
                                    "type": "answer_generation", 
                                    "content": f"æ£€ç´¢å†…å®¹å¯ä»¥å›ç­”é—®é¢˜ï¼ˆç½®ä¿¡åº¦: {judgment.get('confidence', 0):.2f}ï¼‰ï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...",
                                    "timestamp": datetime.now().isoformat()
                                }
                                logger.debug(f"Yielding answer_generation event: {answer_generation_event}")
                                yield answer_generation_event
                                
                                try:
                                    # è§£ææ£€ç´¢ç»“æœ
                                    retrieval_results = self.answer_system.parse_retrieval_results(result)
                                    
                                    # ä½¿ç”¨æµå¼ç”Ÿæˆç­”æ¡ˆ
                                    logger.debug(f"[DEBUG] Starting streaming answer generation...")
                                    
                                    accumulated_answer = ""
                                    answer_data = None
                                    first_chunk = True
                                    
                                    for stream_event in self.answer_system.generate_answer_with_citations_stream(self.user_prompt, retrieval_results):
                                        event_type = stream_event.get("type")
                                        
                                        if event_type == "answer_chunk":
                                            # é€å—å‘é€å¢é‡chunkï¼ˆä»…ç­”æ¡ˆä¸»ä½“ï¼Œä¸å«å‚è€ƒæ–‡çŒ®ï¼‰
                                            chunk_content = stream_event.get("content", "")  # å¢é‡chunkå†…å®¹
                                            accumulated_answer += chunk_content
                                            
                                            # ä½¿ç”¨final_answer_chunkç±»å‹ï¼Œå‰ç«¯ä¼šç”¨æœ€ç»ˆç­”æ¡ˆæ ·å¼æ¸²æŸ“
                                            chunk_event = {
                                                "type": "final_answer_chunk",
                                                "content": chunk_content,  # å‘é€å¢é‡chunkï¼Œè€Œä¸æ˜¯ç´¯ç§¯å†…å®¹
                                                "accumulated": accumulated_answer,  # å¯é€‰ï¼šä¹Ÿæä¾›ç´¯ç§¯å†…å®¹ç»™å‰ç«¯
                                                "is_streaming": True,  # æ ‡è®°ä¸ºæµå¼ä¸­
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            yield chunk_event
                                            
                                        elif event_type == "answer_complete":
                                            # ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œè·å–å®Œæ•´çš„ answer_dataï¼ˆåŒ…å« citationsï¼‰
                                            answer_data = stream_event.get("answer_data", {})
                                            logger.debug(f"[DEBUG] Answer streaming completed, citations count: {len(answer_data.get('citations', []))}")
                                            
                                            # ç›´æ¥ä½¿ç”¨ answer å­—æ®µå†…å®¹ï¼Œä¸è¿›è¡Œæ ¼å¼åŒ–
                                            # accumulated_answer å·²åŒ…å«æµå¼ä¼ è¾“çš„ç­”æ¡ˆä¸»ä½“
                                            final_answer_content = accumulated_answer.strip() if accumulated_answer else answer_data.get("answer", "")
                                            
                                            answer_elapsed = time.time() - answer_start_time
                                            logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {answer_elapsed:.2f} ç§’")
                                            
                                            # ç›´æ¥ä¼ é€’ answer_complete äº‹ä»¶ï¼Œè®©å‰ç«¯ç«‹å³æ˜¾ç¤ºå‚è€ƒæ–‡çŒ®
                                            answer_complete_event = {
                                                "type": "answer_complete",
                                                "content": final_answer_content,  # åªå‘é€ç­”æ¡ˆä¸»ä½“ï¼Œä¸å« "å‚è€ƒæ–‡çŒ®:" æ ¼å¼åŒ–
                                                "answer_data": answer_data,  # å‰ç«¯ä»è¿™é‡Œæå– citations
                                                "is_streaming": False,  # æ ‡è®°æµå¼ç»“æŸ
                                                "timestamp": datetime.now().isoformat(),
                                                "elapsed_time": f"{answer_elapsed:.2f}ç§’"
                                            }
                                            logger.debug(f"Yielding answer_complete event with citations (from retrieval stream)")
                                            yield answer_complete_event
                                            
                                        elif event_type == "answer_error":
                                            # ç­”æ¡ˆç”Ÿæˆå‡ºé”™ - ç«‹å³è¿”å›ï¼Œä¸å†å‘é€completedäº‹ä»¶
                                            error_content = stream_event.get("content", "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™")
                                            error_event = {
                                                "type": "error",
                                                "content": error_content,
                                                "timestamp": datetime.now().isoformat()
                                            }
                                            logger.debug(f"Answer generation error: {error_content}")
                                            yield error_event
                                            
                                            # å‘é€completedäº‹ä»¶åç«‹å³è¿”å›
                                            completed_event = {
                                                "type": "completed",
                                                "content": "ç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œæµç¨‹ç»“æŸ",
                                    "timestamp": datetime.now().isoformat()
                                }
                                            logger.debug(f"Yielding completed event (after error): {completed_event}")
                                            yield completed_event
                                            return  # ç«‹å³è¿”å›ï¼Œé¿å…åç»­å¤„ç†
                                    
                                    # å‘é€å®Œæˆäº‹ä»¶ï¼ˆæ­£å¸¸æµç¨‹ï¼‰
                                    total_elapsed = time.time() - start_time
                                    logger.info(f"â±ï¸  ã€æ—¶é—´ç»Ÿè®¡ã€‘æ•´ä¸ªæµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.2f} ç§’")
                                    
                                    completed_event = {
                                        "type": "completed",
                                        "content": "åŸºäºæ£€ç´¢å†…å®¹ç”Ÿæˆç­”æ¡ˆå®Œæˆ",
                                        "timestamp": datetime.now().isoformat(),
                                        "total_elapsed_time": f"{total_elapsed:.2f}ç§’"
                                    }
                                    logger.debug(f"Yielding completed event (from retrieval): {completed_event}")
                                    yield completed_event
                                    logger.info(f"=== StreamingReactAgent.stream_run COMPLETED (FROM RETRIEVAL) ===")
                                    return
                                    
                                except Exception as e:
                                    error_event = {
                                        "type": "error",
                                        "content": f"ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    logger.debug(f"Error generating final answer from retrieval: {str(e)}")
                                    import traceback
                                    traceback.print_exc()
                                    logger.debug(f"Yielding error event: {error_event}")
                                    yield error_event
                                    
                                    completed_event = {
                                        "type": "completed",
                                        "content": "ç”Ÿæˆç­”æ¡ˆå¤±è´¥ï¼Œæµç¨‹ç»“æŸ",
                                    "timestamp": datetime.now().isoformat()
                                }
                                    logger.debug(f"Yielding completed event (error): {completed_event}")
                                yield completed_event
                                return
                            else:
                                # æ£€ç´¢å†…å®¹ä¸è¶³ï¼Œç»§ç»­æ¨ç†æµç¨‹
                                continue_reasoning_event = {
                                    "type": "continue_reasoning",
                                    "content": f"æ£€ç´¢å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼ˆç½®ä¿¡åº¦: {judgment.get('confidence', 0):.2f}ï¼‰ï¼Œç»§ç»­æ¨ç†æµç¨‹...",
                                    "timestamp": datetime.now().isoformat()
                                }
                                logger.debug(f"Yielding continue_reasoning event: {continue_reasoning_event}")
                                yield continue_reasoning_event
                                
                        except Exception as e:
                            judgment_error_event = {
                                "type": "judgment_error",
                                "content": f"æ£€ç´¢å†…å®¹è¯„ä¼°å‡ºé”™: {str(e)}",
                                "timestamp": datetime.now().isoformat()
                            }
                            logger.debug(f"Yielding judgment_error event: {judgment_error_event}")
                            yield judgment_error_event
                    
                    result = "<tool_response>\n" + result + "\n</tool_response>"
                    messages.append({"role": "user", "content": result})
                    logger.debug(f"Added tool result to conversation")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
                if '<answer>' in content and '</answer>' in content:
                    answer = content.split('<answer>')[1].split('</answer>')[0]
                    final_answer_event = {
                        "type": "final_answer",
                        "content": answer.strip(),
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Yielding final_answer event: {final_answer_event}")
                    yield final_answer_event
                    
                    completed_event = {
                        "type": "completed",
                        "content": "æ¨ç†å®Œæˆ",
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Yielding completed event: {completed_event}")
                    yield completed_event
                    logger.info(f"=== StreamingReactAgent.stream_run COMPLETED ===")
                    return
                    
            except Exception as e:
                error_event = {
                    "type": "error",
                    "content": f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Exception in stream_run: {str(e)}")
                logger.debug(f"Yielding error event: {error_event}")
                yield error_event
                
                completed_event = {
                    "type": "completed",
                    "content": "æ¨ç†é”™è¯¯ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (error): {completed_event}")
                yield completed_event
                logger.info(f"=== StreamingReactAgent.stream_run ERROR ===")
                return
            
            # æ£€æŸ¥tokené™åˆ¶
            max_tokens = 108 * 1024
            token_count = self.count_tokens(messages)
            logger.debug(f"Current token count: {token_count}/{max_tokens}")
            
            if token_count > max_tokens:
                token_limit_event = {
                    "type": "token_limit",
                    "content": f"è¾¾åˆ°tokené™åˆ¶ ({token_count} > {max_tokens})ï¼Œå°è¯•ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding token_limit event: {token_limit_event}")
                yield token_limit_event
                
                messages[-1]['content'] = "You have now reached the maximum context length you can handle. You should stop making tool calls and, based on all the information above, think again and provide what you consider the most likely answer in the following format:<think>your final thinking</think>\n<answer>your answer</answer>"
                
                try:
                    content = self.call_server(messages, planning_port)
                    messages.append({"role": "assistant", "content": content.strip()})
                    
                    if '<answer>' in content and '</answer>' in content:
                        answer = content.split('<answer>')[1].split('</answer>')[0]
                        final_answer_event = {
                            "type": "final_answer",
                            "content": answer.strip(),
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding final_answer event (token limit): {final_answer_event}")
                        yield final_answer_event
                    else:
                        final_answer_event = {
                            "type": "final_answer",
                            "content": content.strip(),
                            "timestamp": datetime.now().isoformat()
                        }
                        logger.debug(f"Yielding final_answer event (token limit, no format): {final_answer_event}")
                        yield final_answer_event
                except Exception as e:
                    error_event = {
                        "type": "error",
                        "content": f"ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}",
                        "timestamp": datetime.now().isoformat()
                    }
                    logger.debug(f"Error generating final answer: {str(e)}")
                    logger.debug(f"Yielding error event: {error_event}")
                    yield error_event
                
                # å‘é€ completed äº‹ä»¶ï¼ˆæ— è®ºæˆåŠŸä¸å¦ï¼‰
                completed_event = {
                    "type": "completed",
                    "content": "Tokené™åˆ¶ï¼Œæµç¨‹ç»“æŸ",
                    "timestamp": datetime.now().isoformat()
                }
                logger.debug(f"Yielding completed event (token limit): {completed_event}")
                yield completed_event
                logger.info(f"=== StreamingReactAgent.stream_run TOKEN_LIMIT_END ===")
                return
                
            round_end_event = {
                "type": "round_end",
                "content": f"ç¬¬ {round_num} è½®æ¨ç†ç»“æŸ",
                "round": round_num,
                "timestamp": datetime.now().isoformat()
            }
            logger.debug(f"Yielding round_end event: {round_end_event}")
            yield round_end_event
        
        # å¦‚æœå¾ªç¯ç»“æŸä»æœªæ‰¾åˆ°ç­”æ¡ˆ
        no_answer_event = {
            "type": "no_answer",
            "content": "æœªæ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ¨ç†è½®æ¬¡",
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Yielding no_answer event: {no_answer_event}")
        yield no_answer_event
        
        # å‘é€ completed äº‹ä»¶
        completed_event = {
            "type": "completed",
            "content": "æ¨ç†å®Œæˆï¼ˆæœªæ‰¾åˆ°ç­”æ¡ˆï¼‰",
            "timestamp": datetime.now().isoformat()
        }
        logger.debug(f"Yielding completed event (no answer): {completed_event}")
        yield completed_event
        logger.info(f"=== StreamingReactAgent.stream_run NO_ANSWER_END ===")



